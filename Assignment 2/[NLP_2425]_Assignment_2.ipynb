{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoman9/NLPProject/blob/main/Assignment%202/%5BNLP_2425%5D_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
        "\n",
        "**Keywords**: Sexism Detection, Multi-class Classification, LLMs, Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFN9Q2rGLJnc"
      },
      "source": [
        "\n",
        "# Contact\n",
        "\n",
        "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "* Eleonora Mancini -> e.mancini@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN4nmQacznGV"
      },
      "source": [
        "# Relevant Material\n",
        "\n",
        "- Tutorial 3\n",
        "- Huggingface documentation\n",
        "- Huggingface hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck47wFhrLJnc"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "You are tasked to address the [EDOS Task A](https://github.com/rewire-online/edos) on sexism detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mtjp0cGLJnc"
      },
      "source": [
        "## Problem definition\n",
        "\n",
        "Given an input text sentence, the task is to label the sentence as sexist or not sexist (binary classification).\n",
        "\n",
        "### Examples:\n",
        "\n",
        "**Text**: *``Schedule a date with her, then don't show up. Then text her \"GOTCHA B___H\".''*\n",
        "\n",
        "**Label**: Sexist\n",
        "\n",
        "**Text**: *``That’s completely ridiculous a woman flashing her boobs is not sexual assault in the slightest.''*\n",
        "\n",
        "**Label**: Not sexist\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dqwRZx-QNGX"
      },
      "source": [
        "## Approach\n",
        "\n",
        "We will tackle the binary classification task with LLMs.\n",
        "\n",
        "In particular, we'll consider zero-/few-shot prompting approaches to assess the capability of some popular open-source LLMs on this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS3igwXpQcAY"
      },
      "source": [
        "## Preliminaries\n",
        "\n",
        "We are going to download LLMs from [Huggingface](https://huggingface.co/).\n",
        "\n",
        "Many of these open-source LLMs require you to accept their \"Community License Agreement\" to download them.\n",
        "\n",
        "In summary:\n",
        "\n",
        "- If not already, create an account of Huggingface (~2 mins)\n",
        "- Check a LLM model card page (e.g., [Mistral v3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)) and accept its \"Community License Agreement\".\n",
        "- Go to your account -> Settings -> Access Tokens -> Create new token -> \"Repositories permissions\" -> add the LLM model card you want to use.\n",
        "- Save the token (we'll need it later)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqEsPH_JSxw6"
      },
      "source": [
        "### Huggingface Login\n",
        "\n",
        "Once we have created an account and an access token, we need to login to Huggingface via code.\n",
        "\n",
        "- Type your token and press Enter\n",
        "- You can say No to Github linking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uWEUjs0THxP"
      },
      "outputs": [],
      "source": [
        "token = \"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\"\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLxSrY-4e_0J"
      },
      "source": [
        "After login, you can download all models associated with your access token in addition to those that are not protected by an access token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEYMBnAQLJnc"
      },
      "source": [
        "### Data Loading\n",
        "\n",
        "Since we are only interested in prompting, we do not require a train dataset.\n",
        "\n",
        "We have preparared a small test set version of EDOS in our dedicated [Github repository](https://github.com/lt-nlp-lab-unibo/nlp-course-material).\n",
        "\n",
        "Check the ``Assignment 2/data`` folder.\n",
        "It contains:\n",
        "\n",
        "- ``a2_test.csv`` → a small test set of 300 samples.\n",
        "- ``demonstrations.csv`` -> a batch of 1000 samples for few-shot prompting.\n",
        "\n",
        "Both datasets contain a balanced number of sexist and not sexist samples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5XyOcFGLJnd"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "We require you to:\n",
        "\n",
        "* **Download** the ``A2/data`` folder.\n",
        "* **Encode** ``a2_test.csv`` into a ``pandas.DataFrame`` object."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Create directory structure if it doesn't exist\n",
        "base_dir = 'Assignment 2/data'\n",
        "if not os.path.exists(base_dir):\n",
        "    os.makedirs(base_dir)\n",
        "\n",
        "# Updated URLs for the raw files on GitHub\n",
        "urls = {\n",
        "    'test': 'https://raw.githubusercontent.com/lt-nlp-lab-unibo/nlp-course-material/main/2024-2025/Assignment%202/data/a2_test.csv',\n",
        "    'demos': 'https://raw.githubusercontent.com/lt-nlp-lab-unibo/nlp-course-material/main/2024-2025/Assignment%202/data/demonstrations.csv'\n",
        "}\n",
        "\n",
        "# Download files\n",
        "for name, url in urls.items():\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        filepath = os.path.join(base_dir, f'{name}.csv')\n",
        "        with open(filepath, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Successfully downloaded {filepath}\")\n",
        "    else:\n",
        "        print(f\"Failed to download {name} dataset. Status code: {response.status_code}\")\n",
        "        print(f\"URL attempted: {url}\")\n",
        "\n",
        "# Verify files exist before loading\n",
        "if os.path.exists(os.path.join(base_dir, 'test.csv')) and os.path.exists(os.path.join(base_dir, 'demos.csv')):\n",
        "    # Load the datasets using pandas\n",
        "\n",
        "    test_df = pd.read_csv(os.path.join(base_dir, 'test.csv'))\n",
        "    demos_df = pd.read_csv(os.path.join(base_dir, 'demos.csv'))\n",
        "\n",
        "    # Display first few rows of test dataset\n",
        "    print(\"\\nFirst few rows of test dataset:\")\n",
        "    print(test_df.head())\n",
        "    # Display basic information about both datasets\n",
        "    print(\"\\nTest dataset info:\")\n",
        "    print(test_df.info())\n",
        "    print(\"\\nDemonstrations dataset info:\")\n",
        "    print(demos_df.info())\n",
        "else:\n",
        "    print(\"\\nFiles were not downloaded successfully. Please check the repository URLs.\")"
      ],
      "metadata": {
        "id": "EyyeGacKsS-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJp08l4yLJnd"
      },
      "source": [
        "# [Task 1 - 0.5 points] Model setup\n",
        "\n",
        "Once the test data has been loaded, we have to setup the model pipeline for inference.\n",
        "\n",
        "In particular, we have to:\n",
        "- Load the model weights from Huggingface\n",
        "- Quantize the model to fit into a single-GPU limited hardware"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptffotFIjq89"
      },
      "source": [
        "## Which LLMs?\n",
        "\n",
        "The pool of LLMs is ever increasing and it's impossible to keep track of all new entries.\n",
        "\n",
        "We focus on popular open-source models.\n",
        "\n",
        "- [Mistral v2](mistralai/Mistral-7B-Instruct-v0.2)\n",
        "- [Mistral v3](mistralai/Mistral-7B-Instruct-v0.3)\n",
        "- [Llama v3.1](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n",
        "- [Phi3-mini](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n",
        "\n",
        "Other open-source models are more than welcome!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH1YShLfLJnd"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "In order to get Task 1 points, we require you to:\n",
        "\n",
        "* Pick 2 model cards from the provided list.\n",
        "* For each model:\n",
        "  - Define a separate section of your notebook for the model.\n",
        "  - Setup a quantization configuration for the model.\n",
        "  - Load the model via HuggingFace APIs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8ORld7klCfG"
      },
      "source": [
        "### Notes\n",
        "\n",
        "1. There's a popular library integrated with Huggingface's ``transformers`` to perform quantization.\n",
        "\n",
        "2. Define two separate sections of your notebook to show that you have implemented the prompting pipeline for each selected model card."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# system packages\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import urllib\n",
        "import tarfile\n",
        "import sys\n",
        "\n",
        "# data and numerical management packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# useful during debugging (progress bars)\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "c5pm0YQbrclr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install accelerate -U\n",
        "!pip install evaluate\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQF5gJtqreHs",
        "outputId": "169fc9f8-5edc-4921-8adb-fd67847948c1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.27.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.12.14)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gqc-CCvRrsmF",
        "outputId": "8c584de6-b89a-4e8c-8bea-56de2f4608bb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4xvu-raruNK",
        "outputId": "0471e01c-273b-4aa2-8ab2-ca12ba23db8a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan  7 17:27:10 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0              30W /  70W |   6491MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "mistral_modelcard = \"mistralai/Mistral-7B-Instruct-v0.3\" # choose mistral model\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_modelcard) # autotokenizer\n",
        "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token # Set the padding token to be the same as the end-of-sequence token\n",
        "\n",
        "\n",
        "# Define special tokens that indicate the end of generation\n",
        "mistral_terminators = [\n",
        "    mistral_tokenizer.eos_token_id,\n",
        "    mistral_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n"
      ],
      "metadata": {
        "id": "YE-TZhuFr3Lz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    mistral_modelcard,\n",
        "    return_dict=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "# Clear CUDA cache\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5d4bc7d1478a478fa75e48ee94711881",
            "6efa1f6cb38440879fdb3ec74de83818",
            "5ef5112380e441e1bb74419e39897e1c",
            "bac9f8624d5a49cdad115aa32aaf9e20",
            "dee48a3b9b824ab485d339ef122df5b0",
            "eb26d2ed52824bd3adb827207532066b",
            "5883fd0b9a9242d8a25ce2a086a781a2",
            "d2f33a5819fc4d6f9e8724178f70a08a",
            "c27550d353dd4e71bcb4ff6325eb4035",
            "52e35cec33e5431fae79cd4f14cd13a4",
            "3fdc7a44702841a5a5050af44e3bfd12"
          ]
        },
        "id": "WWKXHA_nr5B0",
        "outputId": "b01d3bb0-7596-4d45-bf93-d0abd7852601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d4bc7d1478a478fa75e48ee94711881"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phi3_modelcard = \"microsoft/Phi-3-mini-4k-instruct\" # choose phi3 model\n",
        "phi3_tokenizer = AutoTokenizer.from_pretrained(phi3_modelcard, trust_remote_code=True)\n",
        "# Define special tokens that indicate the end of generation\n",
        "phi3_terminators = [\n",
        "    phi3_tokenizer.eos_token_id,\n",
        "    phi3_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]"
      ],
      "metadata": {
        "id": "xZ2K3dJgp4-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "phi3_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "# Clear CUDA cache\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "UecDBAdAjiGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzNNzb1VLJnd"
      },
      "source": [
        "# [Task 2 - 1.0 points] Prompt setup\n",
        "\n",
        "Prompting requires an input pre-processing phase where we convert each input example into a specific instruction prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GaBtKXomY_m"
      },
      "source": [
        "## Prompt Template\n",
        "\n",
        "Use the following prompt template to process input texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e8P-Kk8me6q"
      },
      "outputs": [],
      "source": [
        "prompt = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are an annotator for sexism detection.'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': \"\"\"Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n",
        "\n",
        "        TEXT:\n",
        "        {text}\n",
        "\n",
        "        ANSWER:\n",
        "        \"\"\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHeoEN7MLJnd"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "In order to get Task 2 points, we require you to:\n",
        "\n",
        "* Write a ``prepare_prompts`` function as the one reported below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUjRVMtMm9CE"
      },
      "outputs": [],
      "source": [
        "def prepare_prompts(texts, prompt_template, tokenizer):\n",
        "    \"\"\"\n",
        "    This function formats input text samples into instruction prompts.\n",
        "    \"\"\"\n",
        "    formatted_prompts = []\n",
        "\n",
        "    # Handle both single text and list of texts\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    elif isinstance(texts, pd.Series):\n",
        "        texts = texts.tolist()\n",
        "\n",
        "    for text in texts:\n",
        "        # Create a fresh copy of the template\n",
        "        current_prompt = [\n",
        "            prompt_template[0].copy(),\n",
        "            prompt_template[1].copy()\n",
        "        ]\n",
        "\n",
        "        # Format the user content by replacing the {text} placeholder\n",
        "        current_prompt[1]['content'] = current_prompt[1]['content'].format(text=text)\n",
        "\n",
        "        # Combine system and user messages\n",
        "        full_prompt = f\"{current_prompt[0]['content']}\\n\\n{current_prompt[1]['content']}\"\n",
        "\n",
        "\n",
        "        # Tokenize the formatted prompt\n",
        "        encoded_prompt = tokenizer(\n",
        "            full_prompt,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        formatted_prompts.append(encoded_prompt)\n",
        "\n",
        "    return formatted_prompts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOCRgGf7mifk"
      },
      "source": [
        "### Notes\n",
        "\n",
        "1. You are free to modify the prompt format (**not its content**) as you like depending on your code implementation.\n",
        "\n",
        "2. Note that the provided prompt has placeholders. You need to format the string to replace placeholders. Huggingface might have dedicated APIs for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgBhkBwuLJnd"
      },
      "source": [
        "# [Task 3 - 1.0 points] Inference\n",
        "\n",
        "We are now ready to define the inference loop where we prompt the model with each pre-processed sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WsrQSvcLJnd"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "In order to get Task 3 points, we require you to:\n",
        "\n",
        "* Write a ``generate_responses`` function as the one reported below.\n",
        "* Write a ``process_response`` function as the one reported below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG3CDXNlyD5k"
      },
      "outputs": [],
      "source": [
        "def generate_responses(model, prompt_examples):\n",
        "    \"\"\"\n",
        "    This function implements the inference loop for a LLM model.\n",
        "    Automatically matches {modelname}_model with {modelname}_tokenizer.\n",
        "    \"\"\"\n",
        "    responses = []\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Extract model name from the variable name using globals()\n",
        "    model_vars = [name for name, obj in globals().items() if obj is model]\n",
        "    if not model_vars:\n",
        "        raise ValueError(\"Could not find model variable name in global scope\")\n",
        "\n",
        "    model_name = model_vars[0].replace('_model', '')\n",
        "    tokenizer_name = f\"{model_name}_tokenizer\"\n",
        "\n",
        "    # Get the corresponding tokenizer from globals\n",
        "    if tokenizer_name not in globals():\n",
        "        raise ValueError(f\"Could not find tokenizer '{tokenizer_name}' in global scope\")\n",
        "\n",
        "    tokenizer = globals()[tokenizer_name]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for encoded_prompt in prompt_examples:\n",
        "            # Move tensors to device\n",
        "            input_ids = encoded_prompt['input_ids'].to(device)\n",
        "            attention_mask = encoded_prompt['attention_mask'].to(device)\n",
        "\n",
        "            # Generate response\n",
        "            outputs = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=5,  # Reduced for YES/NO response\n",
        "                do_sample=True,     # Enable sampling\n",
        "                temperature=0.1,    # low temperature\n",
        "                num_beams=3,\n",
        "                pad_token_id=model.config.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "            # Get only the newly generated tokens\n",
        "            new_tokens = outputs[0][input_ids.shape[1]:]\n",
        "            decoded_output = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "            responses.append(decoded_output.strip())\n",
        "\n",
        "            # Clear cache\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return responses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test texts\n",
        "test_texts = [\n",
        "    \"Women belong in the kitchen, not in the workplace.\",  # Clearly sexist\n",
        "    \"Everyone deserves equal opportunities regardless of gender.\",  # Not sexist\n",
        "    \"I like apples.\",  # Neutral\n",
        "    \"She's pretty smart... for a girl.\",  # Sexist\n",
        "    \"The weather is nice today.\"  # Neutral\n",
        "]\n",
        "\n",
        "# Test each text individually\n",
        "for test in test_texts:\n",
        "    print(f\"\\nTesting text: {test}\")\n",
        "    formatted_prompts = prepare_prompts([test], prompt, phi3_tokenizer)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    response = generate_responses(phi3_model, formatted_prompts)\n",
        "    print(f\"Response: {response[0]}\")"
      ],
      "metadata": {
        "id": "NH1HyatKiQ0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiCoMrutyXTU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process_response(response):\n",
        "    \"\"\"\n",
        "    This function processes model responses into binary labels.\n",
        "\n",
        "    Inputs:\n",
        "      response: the raw text response from the model\n",
        "\n",
        "    Outputs:\n",
        "      binary label (sexist/not sexist)\n",
        "    \"\"\"\n",
        "    # Convert response to lowercase for case-insensitive matching\n",
        "    response = response.lower().strip()\n",
        "    print(f\"response: {response}\")\n",
        "    # Check for \"yes\" indicating sexist content\n",
        "    if \"yes\" in response:\n",
        "        return 1\n",
        "    # Check for \"no\" indicating not sexist content\n",
        "    elif \"no\" in response:\n",
        "        return 0\n",
        "    else:\n",
        "        # invalid repsonse\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test:"
      ],
      "metadata": {
        "id": "HpxqDykXs9Yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_random_samples_with_timing(model, tokenizer, test_df, prompt_template, num_samples=10):\n",
        "    results = []\n",
        "    device = next(model.parameters()).device\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Get equal number of samples from both classes\n",
        "    samples_per_class = num_samples // 2\n",
        "\n",
        "    # Get indices for both classes\n",
        "    sexist_indices = test_df[test_df['label_sexist'] == 'sexist'].index\n",
        "    not_sexist_indices = test_df[test_df['label_sexist'] == 'not sexist'].index\n",
        "\n",
        "    # Randomly sample indices from both classes\n",
        "    selected_sexist = random.sample(list(sexist_indices), min(samples_per_class, len(sexist_indices)))\n",
        "    selected_not_sexist = random.sample(list(not_sexist_indices), min(samples_per_class, len(not_sexist_indices)))\n",
        "\n",
        "    # Combine and shuffle the selected indices\n",
        "    selected_indices = selected_sexist + selected_not_sexist\n",
        "    random.shuffle(selected_indices)\n",
        "\n",
        "    # Loop over the selected samples\n",
        "    for i, idx in enumerate(selected_indices):\n",
        "        # Get the sample text and true label\n",
        "        sample_text = test_df.iloc[idx]['text']\n",
        "        true_label = test_df.iloc[idx]['label_sexist']\n",
        "\n",
        "        print(f\"\\nSample {i+1}/{len(selected_indices)}:\")\n",
        "        print(f\"Selected text: {sample_text}\")\n",
        "        print(f\"True label: {true_label}\")\n",
        "\n",
        "        # Format the prompt and generate response\n",
        "        formatted_prompts = prepare_prompts([sample_text], prompt_template, tokenizer)\n",
        "        for j in range(len(formatted_prompts)):\n",
        "            formatted_prompts[j] = {k: v.to(device) for k, v in formatted_prompts[j].items()}\n",
        "\n",
        "        responses = generate_responses(model, formatted_prompts)\n",
        "        binary_label = process_response(responses[0])\n",
        "        predicted_label = \"sexist\" if binary_label == 1 else \"not sexist\"\n",
        "\n",
        "        print(f\"Raw response: {responses[0]}\")\n",
        "        print(f\"Predicted label: {predicted_label}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        # Save result\n",
        "        result = {\n",
        "            'sample_idx': idx,\n",
        "            'text': sample_text,\n",
        "            'true_label': true_label,\n",
        "            'predicted_label': predicted_label,\n",
        "            'raw_response': responses[0],\n",
        "            'correct': predicted_label == true_label\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    # Print summary statistics\n",
        "    correct_predictions = sum(result['correct'] for result in results)\n",
        "    accuracy = correct_predictions / len(results)\n",
        "\n",
        "    sexist_correct = sum(1 for r in results\n",
        "                        if r['true_label'] == 'sexist' and r['correct'])\n",
        "    not_sexist_correct = sum(1 for r in results\n",
        "                            if r['true_label'] == 'not sexist' and r['correct'])\n",
        "\n",
        "    sexist_total = sum(1 for r in results if r['true_label'] == 'sexist')\n",
        "    not_sexist_total = sum(1 for r in results if r['true_label'] == 'not sexist')\n",
        "\n",
        "    print(\"\\nResults Summary:\")\n",
        "    print(f\"Overall Accuracy: {accuracy:.2%}\")\n",
        "    print(f\"Sexist Accuracy: {sexist_correct/sexist_total:.2%} ({sexist_correct}/{sexist_total})\")\n",
        "    print(f\"Not Sexist Accuracy: {not_sexist_correct/not_sexist_total:.2%} ({not_sexist_correct}/{not_sexist_total})\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "WtfWswiss-bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test with first model: mistral v3"
      ],
      "metadata": {
        "id": "ZrKDtdfMs0zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time, random\n",
        "from datetime import timedelta\n",
        "\n",
        "# Run test on n samples\n",
        "tokenizer = mistral_tokenizer\n",
        "start_time = time.time()\n",
        "test_results = test_random_samples_with_timing(mistral_model, tokenizer, test_df, prompt, num_samples=30)\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal execution time: {str(timedelta(seconds=total_time))}\")\n",
        "print(f\"Average time per sample: {str(timedelta(seconds=total_time/100))}\")\n",
        "# Print summary of results\n",
        "correct_predictions = sum(result['correct'] for result in test_results)\n",
        "accuracy = correct_predictions / len(test_results)\n",
        "\n",
        "print(\"\\nTest Summary:\")\n",
        "print(f\"Total samples tested: {len(test_results)}\")\n",
        "print(f\"Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "GQlSo2rYSgbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test with second model: phi3\n"
      ],
      "metadata": {
        "id": "KAeyOXU0s6wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time, random\n",
        "from datetime import timedelta\n",
        "\n",
        "# Run test on n samples\n",
        "start_time = time.time()\n",
        "test_results = test_random_samples_with_timing(phi3_model, phi3_tokenizer, test_df, prompt, num_samples=30)\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal execution time: {str(timedelta(seconds=total_time))}\")\n",
        "print(f\"Average time per sample: {str(timedelta(seconds=total_time/100))}\")\n",
        "# Print summary of results\n",
        "correct_predictions = sum(result['correct'] for result in test_results)\n",
        "accuracy = correct_predictions / len(test_results)\n",
        "\n",
        "print(\"\\nTest Summary:\")\n",
        "print(f\"Total samples tested: {len(test_results)}\")\n",
        "print(f\"Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "ksdvbMqXs9Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKcQ0ZfO2Fya"
      },
      "source": [
        "## Notes\n",
        "\n",
        "1. According to our tests, it should take you ~10 mins to perform full inference on 300 samples."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "92ACmFC9s5vk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyZ8WU09zz-a"
      },
      "source": [
        "# [Task 4 - 0.5 points] Metrics\n",
        "\n",
        "In order to evaluate selected LLMs, we need to compute performance metrics.\n",
        "\n",
        "In particular, we are interested in computing **accuracy** since the provided data is balanced with respect to classification classes.\n",
        "\n",
        "Moreover, we want to compute the ratio of failed responses generated by models.\n",
        "\n",
        "That is, how frequent the LLM fails to follow instructions and provides incorrect responses that do not address the classification task.\n",
        "\n",
        "We denote this metric as **fail-ratio**.\n",
        "\n",
        "In summary, we parse generated responses as follows:\n",
        "- 1 if the model says YES\n",
        "- 0 if the model says NO\n",
        "- 0 if the model does not answer in either way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6lu64o80iX4"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "In order to get Task 4 points, we require you to:\n",
        "\n",
        "* Write a ``compute_metrics`` function as the one reported below.\n",
        "* Compute metrics for the two selected LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Fmcw_9v0k9D"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def compute_metrics(responses, y_true):\n",
        "    \"\"\"\n",
        "    This function takes predicted and ground-truth labels and compute metrics.\n",
        "    In particular, this function compute accuracy and fail-ratio metrics.\n",
        "\n",
        "    Inputs:\n",
        "      responses: generated LLM responses\n",
        "      y_true: ground-truth binary labels (1 for 'sexist', 0 for 'not sexist')\n",
        "\n",
        "    Outputs:\n",
        "      dictionary containing desired metrics\n",
        "    \"\"\"\n",
        "    # Convert string labels to binary if needed\n",
        "    y_true_binary = []\n",
        "    for label in y_true:\n",
        "        if isinstance(label, str):\n",
        "            y_true_binary.append(1 if label == 'sexist' else 0)\n",
        "        else:\n",
        "            y_true_binary.append(label)\n",
        "\n",
        "    # Process responses\n",
        "    y_pred = []\n",
        "    failed_responses = 0\n",
        "    total_responses = len(responses)\n",
        "\n",
        "    for response in responses:\n",
        "        prediction = process_response(response)\n",
        "        if prediction is None:\n",
        "            failed_responses += 1\n",
        "            y_pred.append(0)  # Count failed responses as \"not sexist\"\n",
        "        else:\n",
        "            y_pred.append(prediction)\n",
        "\n",
        "    # Calculate metrics\n",
        "    correct_predictions = sum(1 for true, pred in zip(y_true_binary, y_pred) if true == pred)\n",
        "    accuracy = correct_predictions / total_responses\n",
        "    fail_ratio = failed_responses / total_responses\n",
        "\n",
        "    # Calculate per-class metrics\n",
        "    true_positives = sum(1 for true, pred in zip(y_true_binary, y_pred) if true == 1 and pred == 1)\n",
        "    true_negatives = sum(1 for true, pred in zip(y_true_binary, y_pred) if true == 0 and pred == 0)\n",
        "    false_positives = sum(1 for true, pred in zip(y_true_binary, y_pred) if true == 0 and pred == 1)\n",
        "    false_negatives = sum(1 for true, pred in zip(y_true_binary, y_pred) if true == 1 and pred == 0)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'fail_ratio': fail_ratio,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score,\n",
        "        'confusion_matrix': {\n",
        "            'true_positives': true_positives,\n",
        "            'true_negatives': true_negatives,\n",
        "            'false_positives': false_positives,\n",
        "            'false_negatives': false_negatives\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_with_metrics(model, tokenizer, test_df, prompt_template, num_samples=10):\n",
        "    \"\"\"\n",
        "    Test the model and compute metrics on the results\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    device = next(model.parameters()).device\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Get equal number of samples from both classes\n",
        "    samples_per_class = num_samples // 2\n",
        "\n",
        "    # Get indices for both classes\n",
        "    sexist_indices = test_df[test_df['label_sexist'] == 'sexist'].index\n",
        "    not_sexist_indices = test_df[test_df['label_sexist'] == 'not sexist'].index\n",
        "\n",
        "    # Randomly sample indices from both classes\n",
        "    selected_sexist = random.sample(list(sexist_indices), min(samples_per_class, len(sexist_indices)))\n",
        "    selected_not_sexist = random.sample(list(not_sexist_indices), min(samples_per_class, len(not_sexist_indices)))\n",
        "\n",
        "    # Combine and shuffle\n",
        "    selected_indices = selected_sexist + selected_not_sexist\n",
        "    random.shuffle(selected_indices)\n",
        "\n",
        "    all_responses = []\n",
        "    all_true_labels = []\n",
        "\n",
        "    # Test each sample\n",
        "    for i, idx in enumerate(selected_indices):\n",
        "        sample_text = test_df.iloc[idx]['text']\n",
        "        true_label = test_df.iloc[idx]['label_sexist']\n",
        "\n",
        "        print(f\"\\nSample {i+1}/{len(selected_indices)}:\")\n",
        "        print(f\"Text: {sample_text}\")\n",
        "        print(f\"True label: {true_label}\")\n",
        "\n",
        "        # Generate response\n",
        "        formatted_prompts = prepare_prompts([sample_text], prompt_template, tokenizer)\n",
        "        for j in range(len(formatted_prompts)):\n",
        "            formatted_prompts[j] = {k: v.to(device) for k, v in formatted_prompts[j].items()}\n",
        "\n",
        "        responses = generate_responses(model, formatted_prompts)\n",
        "        print(f\"Model response: {responses[0]}\")\n",
        "\n",
        "        all_responses.append(responses[0])\n",
        "        all_true_labels.append(true_label)\n",
        "\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = compute_metrics(all_responses, all_true_labels)\n",
        "\n",
        "    print(\"\\nFinal Metrics:\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.2%}\")\n",
        "    print(f\"Fail Ratio: {metrics['fail_ratio']:.2%}\")\n",
        "    print(f\"Precision: {metrics['precision']:.2%}\")\n",
        "    print(f\"Recall: {metrics['recall']:.2%}\")\n",
        "    print(f\"F1 Score: {metrics['f1_score']:.2%}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(f\"True Positives: {metrics['confusion_matrix']['true_positives']}\")\n",
        "    print(f\"True Negatives: {metrics['confusion_matrix']['true_negatives']}\")\n",
        "    print(f\"False Positives: {metrics['confusion_matrix']['false_positives']}\")\n",
        "    print(f\"False Negatives: {metrics['confusion_matrix']['false_negatives']}\")\n",
        "\n",
        "    return metrics, all_responses, all_true_labels\n",
        "\n",
        "# Run the test with metrics\n",
        "metrics, responses, true_labels = test_model_with_metrics(model, tokenizer, test_df, prompt, num_samples=10)"
      ],
      "metadata": {
        "id": "Vc1mW3kEqqUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHyvV4QD2vZS"
      },
      "source": [
        "# [Task 5 - 1.0 points] Few-shot Inference\n",
        "\n",
        "So far, we have tested models in a zero-shot fashion: we provide the input text to classify and instruct the model to generate a response.\n",
        "\n",
        "We are now interested in performing few-shot prompting to see the impact of providing demonstration examples.\n",
        "\n",
        "To do so, we slightly change the prompt template as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEqsOHz63ReW"
      },
      "outputs": [],
      "source": [
        "prompt = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are an annotator for sexism detection.'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': \"\"\"Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n",
        "\n",
        "        EXAMPLES:\n",
        "        {examples}\n",
        "\n",
        "        TEXT:\n",
        "        {text}\n",
        "\n",
        "        ANSWER:\n",
        "        \"\"\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SnaxuwN3ySF"
      },
      "source": [
        "The new prompt template reports some demonstration examples to instruct the model.\n",
        "\n",
        "Generally, we provide an equal number of demonstrations per class as shown in the example below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mNmRMQs4VCn"
      },
      "outputs": [],
      "source": [
        "prompt = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are an annotator for sexism detection.'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': \"\"\"Your task is to classify input text as containing sexism or not. Respond only YES or NO.\n",
        "\n",
        "        EXAMPLES:\n",
        "        TEXT: **example 1**\n",
        "        ANSWER: YES\n",
        "        TEXT: **example 2**\n",
        "        ANSWER: NO\n",
        "\n",
        "        TEXT:\n",
        "        {text}\n",
        "\n",
        "        ANSWER:\n",
        "        \"\"\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE0XnaVr3CZz"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "In order to get Task 5 points, we require you to:\n",
        "\n",
        "- Load ``demonstrations.csv`` and encode it into a ``pandas.DataFrame`` object.\n",
        "- Define a ``build_few_shot_demonstrations`` function as the one reported below.\n",
        "- Perform few-shot inference as in Task 3.\n",
        "- Compute metrics as in Task 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oyxypUG3ltH"
      },
      "outputs": [],
      "source": [
        "def build_few_shot_demonstrations(demonstrations, num_per_class=2):\n",
        "  \"\"\"\n",
        "    Inputs:\n",
        "      demonstrations: the pandas.DataFrame object wrapping demonstrations.csv\n",
        "      num_per_class: number of demonstrations per class\n",
        "\n",
        "    Outputs:\n",
        "      a list of textual demonstrations to inject into the prompt template.\n",
        "  \"\"\"\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGpZWQb45XXK"
      },
      "source": [
        "## Notes\n",
        "\n",
        "1. You are free to pick any value for ``num_per_class``.\n",
        "\n",
        "2. According to our tests, few-shot prompting increases inference time by some minutes (we experimented with ``num_per_class`` $\\in [2, 4]$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHuT1a1GLJnd"
      },
      "source": [
        "# [Task 6 - 1.0 points] Error Analysis\n",
        "\n",
        "We are now interested in evaluating model responses and comparing their performance.\n",
        "\n",
        "This analysis helps us in understanding\n",
        "\n",
        "- Classification task performance gap: are the models good at this task?\n",
        "- Generation quality: which kind of responses do models generate?\n",
        "- Errors: which kind of mistakes do models do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kjEAHD4LJne"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "In order to get Task 6 points, we require you to:\n",
        "\n",
        "* Compare classification performance of selected LLMs in a Table.\n",
        "* Compute confusion matrices for selected LLMs.\n",
        "* Briefly summarize your observations on generated responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QWlVXJgLJne"
      },
      "source": [
        "# [Task 7 - 1.0 points] Report\n",
        "\n",
        "Wrap up your experiment in a short report (up to 2 pages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fsdV99TLJne"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Use the NLP course report template.\n",
        "* Summarize each task in the report following the provided template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-hUXYaLLJne"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "The report is not a copy-paste of graphs, tables, and command outputs.\n",
        "\n",
        "* Summarize classification performance in Table format.\n",
        "* **Do not** report command outputs or screenshots.\n",
        "* Report learning curves in Figure format.\n",
        "* The error analysis section should summarize your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fURV8zfPLJne"
      },
      "source": [
        "# Submission\n",
        "\n",
        "* **Submit** your report in PDF format.\n",
        "* **Submit** your python notebook.\n",
        "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn1tUeYzLJne"
      },
      "source": [
        "# FAQ\n",
        "\n",
        "Please check this frequently asked questions before contacting us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYAOVGvKhtTQ"
      },
      "source": [
        "### Model cards\n",
        "\n",
        "You can pick any open-source model card you like.\n",
        "\n",
        "We recommend starting from those reported in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWG72N-LLJne"
      },
      "source": [
        "### Implementation\n",
        "\n",
        "Everything can be done via ``transformers`` APIs.\n",
        "\n",
        "However, you are free to test frameworks, such as [LangChain](https://www.langchain.com/), [LlamaIndex](https://www.llamaindex.ai/) [LitParrot](https://github.com/awesome-software/lit-parrot), provided that you correctly address task instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cplnq3dLJne"
      },
      "source": [
        "### Bonus Points\n",
        "\n",
        "0.5 bonus points are arbitrarily assigned based on significant contributions such as:\n",
        "\n",
        "- Outstanding error analysis\n",
        "- Masterclass code organization\n",
        "- Suitable extensions\n",
        "- Evaluate A1 dataset and perform comparison\n",
        "\n",
        "Note that bonus points are only assigned if all task points are attributed (i.e., 6/6)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coy_pJ40LJne"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "Do not change the provided prompt template.\n",
        "\n",
        "You are only allowed to change it in case of a possible extension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSiH0Xqj79wc"
      },
      "source": [
        "### Optimizations\n",
        "\n",
        "Any kind of code optimization (e.g., speedup model inference or reduce computational cost) is more than welcome!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpr-LSK7LJnh"
      },
      "source": [
        "# The End"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5d4bc7d1478a478fa75e48ee94711881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6efa1f6cb38440879fdb3ec74de83818",
              "IPY_MODEL_5ef5112380e441e1bb74419e39897e1c",
              "IPY_MODEL_bac9f8624d5a49cdad115aa32aaf9e20"
            ],
            "layout": "IPY_MODEL_dee48a3b9b824ab485d339ef122df5b0"
          }
        },
        "6efa1f6cb38440879fdb3ec74de83818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb26d2ed52824bd3adb827207532066b",
            "placeholder": "​",
            "style": "IPY_MODEL_5883fd0b9a9242d8a25ce2a086a781a2",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "5ef5112380e441e1bb74419e39897e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2f33a5819fc4d6f9e8724178f70a08a",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c27550d353dd4e71bcb4ff6325eb4035",
            "value": 0
          }
        },
        "bac9f8624d5a49cdad115aa32aaf9e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52e35cec33e5431fae79cd4f14cd13a4",
            "placeholder": "​",
            "style": "IPY_MODEL_3fdc7a44702841a5a5050af44e3bfd12",
            "value": " 0/3 [00:00&lt;?, ?it/s]"
          }
        },
        "dee48a3b9b824ab485d339ef122df5b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb26d2ed52824bd3adb827207532066b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5883fd0b9a9242d8a25ce2a086a781a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2f33a5819fc4d6f9e8724178f70a08a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c27550d353dd4e71bcb4ff6325eb4035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52e35cec33e5431fae79cd4f14cd13a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fdc7a44702841a5a5050af44e3bfd12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}