{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE4WC2_4wygJ"
      },
      "source": [
        "# Assignment 1\n",
        "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
        "\n",
        "**Keywords**: Sexism Detection, Multi-class Classification, RNNs, Transformers, Huggingface\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL69zGpmx01k"
      },
      "source": [
        "# Contact\n",
        "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "- Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "- Eleonora Mancini -> e.mancini@unibo.it\n",
        "\n",
        "Professor:\n",
        "- Paolo Torroni -> p.torroni@unibo.it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55jnW-xKxi-2"
      },
      "source": [
        "# Introduction\n",
        "You are asked to address the [EXIST 2023 Task 1](https://clef2023.clef-initiative.eu/index.php?page=Pages/labs.html#EXIST) on sexism detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HWp5bGwySsb"
      },
      "source": [
        "## Problem Definition\n",
        "The systems have to decide whether or not a given tweet contains or describes sexist expressions or behaviors (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behavior).\n",
        "\n",
        "### Examples:\n",
        "\n",
        "**Text**: *Can’t go a day without women womening*\n",
        "\n",
        "**Label**: Sexist\n",
        "\n",
        "**Text**: *''Society's set norms! Happy men's day though!#weareequal''*\n",
        "\n",
        "**Label**: Not sexist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iu1X4I98M8B"
      },
      "source": [
        "# [Task 1 - 1.0 points] Corpus\n",
        "\n",
        "We have preparared a small version of EXIST dataset in our dedicated [Github repository](https://github.com/lt-nlp-lab-unibo/nlp-course-material/tree/main/2024-2025/Assignment%201/data).\n",
        "\n",
        "Check the `A1/data` folder. It contains 3 `.json` files representing `training`, `validation` and `test` sets.\n",
        "\n",
        "The three sets are slightly unbalanced, with a bias toward the `Non-sexist` class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AASoMV9XN5l6"
      },
      "source": [
        "### Dataset Description\n",
        "- The dataset contains tweets in both English and Spanish.\n",
        "- There are labels for multiple tasks, but we are focusing on **Task 1**.\n",
        "- For Task 1, soft labels are assigned by six annotators.\n",
        "- The labels for Task 1 represent whether the tweet is sexist (\"YES\") or not (\"NO\").\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFjwB_lCOQKj"
      },
      "source": [
        "\n",
        "### Example\n",
        "\n",
        "\n",
        "    \"203260\": {\n",
        "        \"id_EXIST\": \"203260\",\n",
        "        \"lang\": \"en\",\n",
        "        \"tweet\": \"ik when mandy says “you look like a whore” i look cute as FUCK\",\n",
        "        \"number_annotators\": 6,\n",
        "        \"annotators\": [\"Annotator_473\", \"Annotator_474\", \"Annotator_475\", \"Annotator_476\", \"Annotator_477\", \"Annotator_27\"],\n",
        "        \"gender_annotators\": [\"F\", \"F\", \"M\", \"M\", \"M\", \"F\"],\n",
        "        \"age_annotators\": [\"18-22\", \"23-45\", \"18-22\", \"23-45\", \"46+\", \"46+\"],\n",
        "        \"labels_task1\": [\"YES\", \"YES\", \"YES\", \"NO\", \"YES\", \"YES\"],\n",
        "        \"labels_task2\": [\"DIRECT\", \"DIRECT\", \"REPORTED\", \"-\", \"JUDGEMENTAL\", \"REPORTED\"],\n",
        "        \"labels_task3\": [\n",
        "          [\"STEREOTYPING-DOMINANCE\"],\n",
        "          [\"OBJECTIFICATION\"],\n",
        "          [\"SEXUAL-VIOLENCE\"],\n",
        "          [\"-\"],\n",
        "          [\"STEREOTYPING-DOMINANCE\", \"OBJECTIFICATION\"],\n",
        "          [\"OBJECTIFICATION\"]\n",
        "        ],\n",
        "        \"split\": \"TRAIN_EN\"\n",
        "      }\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ45bvuOOJ7I"
      },
      "source": [
        "### Instructions\n",
        "1. **Download** the `A1/data` folder.\n",
        "2. **Load** the three JSON files and encode them as pandas dataframes.\n",
        "3. **Generate hard labels** for Task 1 using majority voting and store them in a new dataframe column called `hard_label_task1`. Items without a clear majority will be removed from the dataset.\n",
        "4. **Filter the DataFrame** to keep only rows where the `lang` column is `'en'`.\n",
        "5. **Remove unwanted columns**: Keep only `id_EXIST`, `lang`, `tweet`, and `hard_label_task1`.\n",
        "6. **Encode the `hard_label_task1` column**: Use 1 to represent \"YES\" and 0 to represent \"NO\"."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgtU1jX_R9Hm",
        "outputId": "073dd78e-95d2-4310-bfe7-e1b2a28f31ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/586.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m583.7/586.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspellchecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F5FpADuuJK2",
        "outputId": "416c916a-a019-43e7-c11c-dc83b453314f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/7.1 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/7.1 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHgKtVTh06JX",
        "outputId": "d6ad5ef9-5ccf-481e-9a9b-09d5c0811772"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcksoLekLVgc"
      },
      "source": [
        "### 2. Loading of the three JSON files as dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8LUWW3uMLVgc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#data_train = pd.read_json('data/training.json')\n",
        "#data_train = pd.read_json('content/training.json') #Lorenzo\n",
        "data_train = pd.read_json('/content/training.json') #Diego\n",
        "\n",
        "\n",
        "df_train = data_train.T\n",
        "df_train.reset_index(inplace=True)\n",
        "df_train.rename(columns={'index': 'id'}, inplace=True)\n",
        "\n",
        "#data_test = pd.read_json('data/test.json')\n",
        "#data_test = pd.read_json('content/test.json') #Lorenzo\n",
        "data_test = pd.read_json('/content/test.json') #Diego\n",
        "\n",
        "df_test = data_test.T\n",
        "df_test.reset_index(inplace=True)\n",
        "df_test.rename(columns={'index': 'id'}, inplace=True)\n",
        "\n",
        "#data_val = pd.read_json('data/validation.json')\n",
        "#data_val = pd.read_json('content/validation.json') #Lorenzo\n",
        "data_val = pd.read_json('/content/validation.json') #Diego\n",
        "\n",
        "\n",
        "df_val = data_val.T\n",
        "df_val.reset_index(inplace=True)\n",
        "df_val.rename(columns={'index': 'id'}, inplace=True)\n",
        "\n",
        "# DataFrame displaying\n",
        "#print(df_train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10HueBHELVgc"
      },
      "source": [
        "### 3. Generate hard_label_task1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qh7mDbtoLVgc"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def majority_vote(labels):\n",
        "    label_counts = Counter(labels)\n",
        "    most_common = label_counts.most_common(1)\n",
        "    if len(most_common) > 0 and most_common[0][1] > len(labels) / 2:\n",
        "        return most_common[0][0]\n",
        "    return None\n",
        "\n",
        "df_train['hard_label_task1'] = df_train['labels_task1'].apply(majority_vote)\n",
        "df_train = df_train.dropna(subset=['hard_label_task1'])\n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df_test['hard_label_task1'] = df_test['labels_task1'].apply(majority_vote)\n",
        "df_test = df_test.dropna(subset=['hard_label_task1'])\n",
        "df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df_val['hard_label_task1'] = df_val['labels_task1'].apply(majority_vote)\n",
        "df_val = df_val.dropna(subset=['hard_label_task1'])\n",
        "df_val.reset_index(drop=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R6OaY1ALVgc"
      },
      "source": [
        "### 4. Filter DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gi_FsJDQLVgc"
      },
      "outputs": [],
      "source": [
        "df_train = df_train[df_train['lang'] == 'en']\n",
        "df_test = df_test[df_test['lang'] == 'en']\n",
        "df_val = df_val[df_val['lang'] == 'en']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLLeN79OLVgd"
      },
      "source": [
        "### 5. Remove Unwanted Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fBLTFgNuLVgd"
      },
      "outputs": [],
      "source": [
        "df_train = df_train[['id_EXIST', 'lang', 'tweet', 'hard_label_task1']]\n",
        "df_test = df_test[['id_EXIST', 'lang', 'tweet', 'hard_label_task1']]\n",
        "df_val = df_val[['id_EXIST', 'lang', 'tweet', 'hard_label_task1']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RFvA37OLVgd"
      },
      "source": [
        "### 6. hard_label_task1 encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZND3Mu40LVgd",
        "outputId": "a2c45f74-e4c6-457d-efc7-2fc9fe8da9bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-956ecd6188b9>:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df_train['hard_label_task1'] = df_train['hard_label_task1'].replace({'YES': 1, 'NO': 0})\n",
            "<ipython-input-9-956ecd6188b9>:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df_test['hard_label_task1'] = df_test['hard_label_task1'].replace({'YES': 1, 'NO': 0})\n",
            "<ipython-input-9-956ecd6188b9>:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df_val['hard_label_task1'] = df_val['hard_label_task1'].replace({'YES': 1, 'NO': 0})\n"
          ]
        }
      ],
      "source": [
        "df_train['hard_label_task1'] = df_train['hard_label_task1'].replace({'YES': 1, 'NO': 0})\n",
        "df_test['hard_label_task1'] = df_test['hard_label_task1'].replace({'YES': 1, 'NO': 0})\n",
        "df_val['hard_label_task1'] = df_val['hard_label_task1'].replace({'YES': 1, 'NO': 0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "RyrGidPaLVgd"
      },
      "source": [
        "# [Task2 - 0.5 points] Data Cleaning\n",
        "In the context of tweets, we have noisy and informal data that often includes unnecessary elements like emojis, hashtags, mentions, and URLs. These elements may interfere with the text analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aiOvW7DnLVgd"
      },
      "source": [
        "\n",
        "### Instructions\n",
        "- **Remove emojis** from the tweets.\n",
        "- **Remove hashtags** (e.g., `#example`).\n",
        "- **Remove mentions** such as `@user`.\n",
        "- **Remove URLs** from the tweets.\n",
        "- **Remove special characters and symbols**.\n",
        "- **Remove specific quote characters** (e.g., curly quotes).\n",
        "- **Perform lemmatization** to reduce words to their base form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejGvUvoNzq0p",
        "outputId": "bc6dc4aa-cc0f-440f-88f9-d7dc2d610d10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_key(pos_tag):\n",
        "    if pos_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif pos_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif pos_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif pos_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return 'n'\n",
        "\n",
        "#Lemmatize each token\n",
        "def lem_text(text: str):\n",
        "  tokens = word_tokenize(text)\n",
        "  tagged = pos_tag(tokens)\n",
        "  words = [lemmatizer.lemmatize(word, get_wordnet_key(tag)) for word, tag in tagged]\n",
        "  return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgXFO2RlLVgd",
        "outputId": "2bfe076f-01ee-4980-99de-6cec43982723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "100%|██████████| 2870/2870 [00:08<00:00, 355.50it/s]\n",
            "100%|██████████| 158/158 [00:00<00:00, 367.08it/s]\n",
            "100%|██████████| 286/286 [00:00<00:00, 377.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  tweet  \\\n",
            "3194  Writing a uni essay in my local pub with a cof...   \n",
            "3195  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
            "3196  According to a customer I have plenty of time ...   \n",
            "3197  So only 'blokes' drink beer? Sorry, but if you...   \n",
            "3198  New to the shelves this week - looking forward...   \n",
            "\n",
            "                                          cleaned_tweet  \n",
            "3194  writing a uni essay in my local pub with a cof...  \n",
            "3195  it be 2021 not 1921 i dont appreciate that on ...  \n",
            "3196  according to a customer i have plenty of time ...  \n",
            "3197  so only blokes drink beer sorry but if you are...  \n",
            "3198  new to the shelf this week look forward to rea...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import emoji\n",
        "import re\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "import unicodedata\n",
        "from textblob import TextBlob\n",
        "#from nltk.tokenize import word_tokenize\n",
        "from emoji import *\n",
        "from nltk import word_tokenize\n",
        "\n",
        "\n",
        "# necessary for being able to tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def correct_spelling(text):\n",
        "  blob = TextBlob(text)\n",
        "  corrected_text = blob.correct()\n",
        "  return str(corrected_text)\n",
        "\n",
        "def remove_style(text):\n",
        "    # Normalize text into the closest ASCII equivalent\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFKC', text)\n",
        "        if not unicodedata.combining(c)  # Exclude combining marks\n",
        "    )\n",
        "\n",
        "def split_merge_word(text):\n",
        "    # Use regex to find boundaries between lowercase and uppercase\n",
        "    return re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)\n",
        "\n",
        "def replace_space(text):\n",
        "  return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# Function to clean and preprocess tweets\n",
        "def clean_tweet(tweet):\n",
        "    # Remove mentions (@user)\n",
        "    tweet = re.sub(r'@\\w+', ' ', tweet)\n",
        "    # Remove hashtags (#example)\n",
        "    tweet = re.sub(r'#\\w+', ' ', tweet)\n",
        "    # Remove URLs\n",
        "    tweet = re.sub(r'http\\S+|www.\\S+', ' ', tweet)\n",
        "    # Remove special characters and symbols\n",
        "    tweet = re.sub(r'[^\\w\\s]', ' ', tweet)\n",
        "    # Remove emojis\n",
        "    tweet = replace_emoji(tweet, ' ')\n",
        "    # Remove specific quote characters (e.g., curly quotes)\n",
        "    cleaned_tweet = tweet.replace('“', ' ').replace('”', ' ').replace('’', \" \").replace(\"‘\",\" \").replace('\"', \" \").replace(\"'\", \" \")\n",
        "\n",
        "    return cleaned_tweet\n",
        "\n",
        "def clean_column_dataset(df_column):\n",
        "  cleaned_tweets = []\n",
        "\n",
        "  for tweet in tqdm(df_column):\n",
        "    cleaned_tweet = clean_tweet(tweet)   #Clean the text\n",
        "    lem_tweet = lem_text(cleaned_tweet) #Lemmatize the text\n",
        "    lem_tweet_split = split_merge_word(lem_tweet) #Split words like \"endYou\"\n",
        "    norm_tweet = remove_style(lem_tweet_split) #remove bold and italic style\n",
        "    lowercase_tweet = norm_tweet.lower() #lower case the dataset\n",
        "    cleaned_tweets.append(lowercase_tweet)  #Save the results\n",
        "  return cleaned_tweets\n",
        "\n",
        "df_train['cleaned_tweet'] = clean_column_dataset(df_train['tweet'])\n",
        "df_val['cleaned_tweet'] = clean_column_dataset(df_val['tweet'])\n",
        "df_test['cleaned_tweet'] = clean_column_dataset(df_test['tweet'])\n",
        "\n",
        "\n",
        "print(df_train[['tweet', 'cleaned_tweet']].head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[['tweet', 'cleaned_tweet']].head(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "CWEEv5Yae9nB",
        "outputId": "9cb086cf-e965-410c-e547-0e3fc8701915"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  tweet  \\\n",
              "3194  Writing a uni essay in my local pub with a cof...   \n",
              "3195  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
              "3196  According to a customer I have plenty of time ...   \n",
              "3197  So only 'blokes' drink beer? Sorry, but if you...   \n",
              "3198  New to the shelves this week - looking forward...   \n",
              "...                                                 ...   \n",
              "3289  They can fight for Hijabs but not against Trip...   \n",
              "3290  The whiskey and cigars, the $99 dollar seminar...   \n",
              "3291  I would be glad to see the violence on Twitter...   \n",
              "3292  @ProteanRedux @HugoThePinkCat @DeclarationOn @...   \n",
              "3293  Over 400 people used our #streetharassment hel...   \n",
              "\n",
              "                                          cleaned_tweet  \n",
              "3194  writing a uni essay in my local pub with a cof...  \n",
              "3195  it be 2021 not 1921 i dont appreciate that on ...  \n",
              "3196  according to a customer i have plenty of time ...  \n",
              "3197  so only blokes drink beer sorry but if you are...  \n",
              "3198  new to the shelf this week look forward to rea...  \n",
              "...                                                 ...  \n",
              "3289  they can fight for hijabs but not against trip...  \n",
              "3290  the whiskey and cigars the 99 dollar seminar t...  \n",
              "3291  i would be glad to see the violence on twitter...  \n",
              "3292  action aid conduct a survey on street harassme...  \n",
              "3293  over 400 people use our helpline this year you...  \n",
              "\n",
              "[100 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c381107-336f-46ac-be8c-a16bb460e253\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>cleaned_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3194</th>\n",
              "      <td>Writing a uni essay in my local pub with a cof...</td>\n",
              "      <td>writing a uni essay in my local pub with a cof...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3195</th>\n",
              "      <td>@UniversalORL it is 2021 not 1921. I dont appr...</td>\n",
              "      <td>it be 2021 not 1921 i dont appreciate that on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3196</th>\n",
              "      <td>According to a customer I have plenty of time ...</td>\n",
              "      <td>according to a customer i have plenty of time ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3197</th>\n",
              "      <td>So only 'blokes' drink beer? Sorry, but if you...</td>\n",
              "      <td>so only blokes drink beer sorry but if you are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3198</th>\n",
              "      <td>New to the shelves this week - looking forward...</td>\n",
              "      <td>new to the shelf this week look forward to rea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3289</th>\n",
              "      <td>They can fight for Hijabs but not against Trip...</td>\n",
              "      <td>they can fight for hijabs but not against trip...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3290</th>\n",
              "      <td>The whiskey and cigars, the $99 dollar seminar...</td>\n",
              "      <td>the whiskey and cigars the 99 dollar seminar t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3291</th>\n",
              "      <td>I would be glad to see the violence on Twitter...</td>\n",
              "      <td>i would be glad to see the violence on twitter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3292</th>\n",
              "      <td>@ProteanRedux @HugoThePinkCat @DeclarationOn @...</td>\n",
              "      <td>action aid conduct a survey on street harassme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3293</th>\n",
              "      <td>Over 400 people used our #streetharassment hel...</td>\n",
              "      <td>over 400 people use our helpline this year you...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c381107-336f-46ac-be8c-a16bb460e253')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8c381107-336f-46ac-be8c-a16bb460e253 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8c381107-336f-46ac-be8c-a16bb460e253');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c38cb3ae-bec3-415d-b2bf-fc6a558bd33d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c38cb3ae-bec3-415d-b2bf-fc6a558bd33d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c38cb3ae-bec3-415d-b2bf-fc6a558bd33d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_train[['tweet', 'cleaned_tweet']]\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"tweet\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"An excellent thread on how the patriarchy deforms men's souls and damages their lives. https://t.co/hQ16C565CT\",\n          \"black femcels, nigcels, hoteps, d!vestors, femininity nuts, black luxury ppl, black mgtow &amp; other online groups fascinate me LOLthe way they twist legitimate ideas abt race and oppression to fit their catastrophizing worldview + inform behavior that leads them back to ws is\\u2026\",\n          \"Any person putting pronouns in their bio and chanting TWAW is saying #NotAllMen. This is not \\u2018anti\\u2019 anyone - it\\u2019s recognising that in order to address the entrenched misogyny, the abuse and violence that is inflicted on women we have to recognise sex. #sexmatters\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_tweet\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"an excellent thread on how the patriarchy deform men s soul and damage their life\",\n          \"black femcels nigcels hoteps d vestors femininity nut black luxury ppl black mgtow amp other online group fascinate me lolthe way they twist legitimate idea abt race and oppression to fit their catastrophizing worldview inform behavior that lead them back to w be\",\n          \"any person put pronoun in their bio and chant twaw be say this be not anti anyone it s recognise that in order to address the entrenched misogyny the abuse and violence that be inflict on woman we have to recognise sex\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3KylLHNl0bE"
      },
      "source": [
        "# [Task 3 - 0.5 points] Text Encoding\n",
        "To train a neural sexism classifier, you first need to encode text into numerical format.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr1lTHUVOXff"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Embed words using **GloVe embeddings**.\n",
        "* You are **free** to pick any embedding dimension.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "from collections import OrderedDict\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "def build_vocabulary(df: pd.DataFrame) -> (Dict[int, str], Dict[str, int], List[str]):\n",
        "    \"\"\"\n",
        "    Given a dataset, builds the corresponding word vocabulary.\n",
        "\n",
        "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
        "    :return:\n",
        "      - word vocabulary: vocabulary index to word\n",
        "      - inverse word vocabulary: word to vocabulary index\n",
        "      - word listing: set of unique terms that build up the vocabulary\n",
        "    \"\"\"\n",
        "    idx_to_word = OrderedDict()\n",
        "    word_to_idx = OrderedDict()\n",
        "\n",
        "    curr_idx = 0\n",
        "    for sentence in tqdm(df.text.values):\n",
        "        tokens = sentence.split()\n",
        "        for token in tokens:\n",
        "            if token not in word_to_idx:\n",
        "                word_to_idx[token] = curr_idx\n",
        "                idx_to_word[curr_idx] = token\n",
        "                curr_idx += 1\n",
        "\n",
        "    word_to_idx[\"UNK\"] = curr_idx\n",
        "    idx_to_word[curr_idx] = 'UNK'\n",
        "\n",
        "    word_listing = list(idx_to_word.values())\n",
        "    return idx_to_word, word_to_idx, word_listing"
      ],
      "metadata": {
        "id": "sWmfN9z39qnl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['text'] = df_train['cleaned_tweet']\n",
        "df_test['text'] = df_test['cleaned_tweet']\n",
        "df_val['text'] = df_val['cleaned_tweet']"
      ],
      "metadata": {
        "id": "GTQ_eosEXTMH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_word_train, word_to_idx_train, word_listing_train = build_vocabulary(df_train)\n",
        "idx_to_word_test, word_to_idx_test, word_listing_test = build_vocabulary(df_test)\n",
        "idx_to_word_val, word_to_idx_val, word_listing_val = build_vocabulary(df_val)"
      ],
      "metadata": {
        "id": "Rc0vWCZ891dY",
        "outputId": "81683eb4-825e-4ac0-9f4a-b113f7a27b85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2870/2870 [00:00<00:00, 93073.43it/s]\n",
            "100%|██████████| 286/286 [00:00<00:00, 62493.93it/s]\n",
            "100%|██████████| 158/158 [00:00<00:00, 47400.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_vocabulary(idx_to_word: Dict[int, str], word_to_idx: Dict[str, int],\n",
        "                        word_listing: List[str], df: pd.DataFrame, check_default_size: bool = False):\n",
        "    print(\"[Vocabulary Evaluation] Size checking...\")\n",
        "    assert len(idx_to_word) == len(word_to_idx)\n",
        "    assert len(idx_to_word) == len(word_listing)\n",
        "\n",
        "    print(\"[Vocabulary Evaluation] Content checking...\")\n",
        "    for i in tqdm(range(0, len(idx_to_word))):\n",
        "        assert idx_to_word[i] in word_to_idx\n",
        "        assert word_to_idx[idx_to_word[i]] == i\n",
        "\n",
        "    print(\"[Vocabulary Evaluation] Consistency checking...\")\n",
        "    _, _, first_word_listing = build_vocabulary(df)\n",
        "    _, _, second_word_listing = build_vocabulary(df)\n",
        "    assert first_word_listing == second_word_listing\n",
        "\n",
        "    print(\"[Vocabulary Evaluation] Toy example checking...\")\n",
        "    toy_df = pd.DataFrame.from_dict({\n",
        "        'text': [\"all that glitters is not gold\", \"all in all i like this assignment\"]\n",
        "    })\n",
        "    _, _, toy_word_listing = build_vocabulary(toy_df)\n",
        "    toy_valid_vocabulary = set(' '.join(toy_df.text.values).split())\n",
        "    assert set(toy_word_listing) == toy_valid_vocabulary"
      ],
      "metadata": {
        "id": "wZiZcrgLByL8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary evaluation...\")\n",
        "evaluate_vocabulary(idx_to_word_train, word_to_idx_train, word_listing_train, df_train)\n",
        "print(\"Evaluation completed!\")"
      ],
      "metadata": {
        "id": "kMhMLwmEBwA-",
        "outputId": "0dc2ef31-0f99-45b9-ce2b-585972643ac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary evaluation...\n",
            "[Vocabulary Evaluation] Size checking...\n",
            "[Vocabulary Evaluation] Content checking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9388/9388 [00:00<00:00, 633718.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Vocabulary Evaluation] Consistency checking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2870/2870 [00:00<00:00, 153181.98it/s]\n",
            "100%|██████████| 2870/2870 [00:00<00:00, 144812.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Vocabulary Evaluation] Toy example checking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 11554.56it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-10931f510cd6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary evaluation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_to_word_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_idx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_listing_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluation completed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e504df6fd3da>\u001b[0m in \u001b[0;36mevaluate_vocabulary\u001b[0;34m(idx_to_word, word_to_idx, word_listing, df, check_default_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoy_word_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoy_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtoy_valid_vocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoy_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoy_word_listing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtoy_valid_vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AIoWFwyAFmIg",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "\n",
        "def load_embedding_glove_model(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = load_embedding_glove_model(embedding_dimension=50)"
      ],
      "metadata": {
        "id": "cl9SZWLaCEYA",
        "outputId": "1d1c6bb4-1818-49b0-a1d2-47ddf8ef726f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6NNMEjWOZQr"
      },
      "source": [
        "### Note : What about OOV tokens?\n",
        "   * All the tokens in the **training** set that are not in GloVe **must** be added to the vocabulary.\n",
        "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **special token** (e.g., [UNK]) and a **static** embedding.\n",
        "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90UztlGUObXk"
      },
      "source": [
        "### More about OOV\n",
        "\n",
        "For a given token:\n",
        "\n",
        "* **If in train set**: add to vocabulary and assign an embedding (use GloVe if token in GloVe, custom embedding otherwise).\n",
        "* **If in val/test set**: assign special token if not in vocabulary and assign custom embedding.\n",
        "\n",
        "Your vocabulary **should**:\n",
        "\n",
        "* Contain all tokens in train set; or\n",
        "* Union of tokens in train set and in GloVe $\\rightarrow$ we make use of existing knowledge!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                    word_listing: List[str]):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    embedding_vocabulary = set(embedding_model.key_to_index.keys())\n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)"
      ],
      "metadata": {
        "id": "Csy7Tm7R6QIf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov_terms_train = check_OOV_terms(embedding_model, word_listing_train)\n",
        "oov_percentage_train = float(len(oov_terms_train)) * 100 / len(word_listing_train)\n",
        "print(f\"Total OOV terms: {len(oov_terms_train)} ({oov_percentage_train:.2f}%)\")\n",
        "\n",
        "oov_terms_test = check_OOV_terms(embedding_model, word_listing_test)\n",
        "oov_percentage_test = float(len(oov_terms_test)) * 100 / len(word_listing_test)\n",
        "print(f\"Total OOV terms: {len(oov_terms_test)} ({oov_percentage_test:.2f}%)\")\n",
        "\n",
        "oov_terms_val = check_OOV_terms(embedding_model, word_listing_val)\n",
        "oov_percentage_val = float(len(oov_terms_val)) * 100 / len(word_listing_val)\n",
        "print(f\"Total OOV terms: {len(oov_terms_val)} ({oov_percentage_val:.2f}%)\")"
      ],
      "metadata": {
        "id": "8FlfIz-W6Yy6",
        "outputId": "20fb7607-01c1-4fbd-f0d3-570b8a65443f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total OOV terms: 880 (9.37%)\n",
            "Total OOV terms: 102 (4.72%)\n",
            "Total OOV terms: 68 (4.42%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for oov in oov_terms_train:\n",
        "  print(oov)"
      ],
      "metadata": {
        "id": "6b-pX6rsQ9JM",
        "outputId": "4a55e9f3-ea01-4058-99a9-3289128b1b04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "collab\n",
            "5days\n",
            "twtjesus\n",
            "myt\n",
            "motuary\n",
            "likeyou\n",
            "dvmb\n",
            "ekyo\n",
            "deadass\n",
            "flyering\n",
            "suguha\n",
            "carousel30\n",
            "godess\n",
            "antifa\n",
            "erens\n",
            "jtrump\n",
            "dforce\n",
            "homosapiens\n",
            "otherized\n",
            "weightlift\n",
            "chaturbate\n",
            "wetin\n",
            "wowsers\n",
            "scoatlan\n",
            "peacefuls\n",
            "000s\n",
            "trynna\n",
            "aytl3\n",
            "gimbap\n",
            "30yo\n",
            "mansplain\n",
            "tinnie\n",
            "porngirl\n",
            "smtreading\n",
            "fkn\n",
            "qristianity\n",
            "lanez\n",
            "fatphobic\n",
            "findomgoddess\n",
            "transness\n",
            "notallmen\n",
            "paperswritingpro\n",
            "talaga\n",
            "来自\n",
            "gooning\n",
            "inmoral\n",
            "wachana\n",
            "38original\n",
            "big0ted\n",
            "journo\n",
            "amrylin\n",
            "country10\n",
            "p5srm7ai\n",
            "senary\n",
            "transphobes\n",
            "shiiit\n",
            "oloriburuku\n",
            "gcfems\n",
            "dababy\n",
            "wgtn\n",
            "yoohan\n",
            "yonke\n",
            "purebloods\n",
            "anyhoo\n",
            "pathologises\n",
            "hoseok\n",
            "humberman\n",
            "pandra\n",
            "yrboi\n",
            "adorime\n",
            "rendelshem\n",
            "yooooooo\n",
            "loooose\n",
            "imho\n",
            "karke\n",
            "bakubussy\n",
            "bhululu\n",
            "trumpanzees\n",
            "hooooly\n",
            "rewatching\n",
            "nonary\n",
            "catholism\n",
            "bitvh\n",
            "chefayo\n",
            "fffffffuck\n",
            "overfeed\n",
            "yeee\n",
            "brotopia\n",
            "fakesswi\n",
            "dvmbses\n",
            "truefender\n",
            "maaaan\n",
            "ọnịcha\n",
            "itseems\n",
            "sherni\n",
            "othering\n",
            "quinary\n",
            "65purchase\n",
            "lalalala\n",
            "pomlázka\n",
            "wokeness\n",
            "immerwahr\n",
            "pppl\n",
            "wh0r3s\n",
            "sheisty\n",
            "lxp\n",
            "bowcaster\n",
            "moots10\n",
            "cker\n",
            "ranpo\n",
            "poet2021\n",
            "penised\n",
            "wildturtle\n",
            "cishet\n",
            "futanari\n",
            "65k\n",
            "mpeleke\n",
            "vaxxers\n",
            "quemix\n",
            "octonary\n",
            "vestors\n",
            "covid2\n",
            "cringy\n",
            "phallocentric\n",
            "akaabir\n",
            "icymi\n",
            "gfy\n",
            "wabout\n",
            "philiacs\n",
            "mysogynistic\n",
            "axiomatizes\n",
            "p4nties\n",
            "leapable\n",
            "ortak\n",
            "porsha\n",
            "questuons\n",
            "iwd\n",
            "wvst\n",
            "ughhh\n",
            "ᅠhina\n",
            "referreing\n",
            "kelangan\n",
            "oppressedsuch\n",
            "wabt\n",
            "lillets\n",
            "radness\n",
            "infantilise\n",
            "ᴗ\n",
            "selfawareness\n",
            "choor\n",
            "mfer\n",
            "enby\n",
            "ochako\n",
            "mtgs\n",
            "victimsand\n",
            "zunelhak\n",
            "ep01\n",
            "amadudu\n",
            "malewife\n",
            "ds5k\n",
            "youuu\n",
            "sosatie\n",
            "positionality\n",
            "dosent\n",
            "rvmt1d\n",
            "becomming\n",
            "sasukr\n",
            "idrk\n",
            "wendyallure\n",
            "waaaa\n",
            "sntiques\n",
            "transfems\n",
            "funfetti\n",
            "himbo\n",
            "brokeside\n",
            "depency\n",
            "tansume\n",
            "90k\n",
            "mkk\n",
            "lustwhat\n",
            "ngang\n",
            "exoticals\n",
            "bahahah\n",
            "3762\n",
            "ducttape\n",
            "kaizenix\n",
            "murrrphhhur\n",
            "youtoo\n",
            "prolly\n",
            "polititcs\n",
            "ọkpọsị\n",
            "gronru\n",
            "thickhead\n",
            "chomos\n",
            "xrp\n",
            "b00ty\n",
            "archipiélago\n",
            "nyjc\n",
            "obvs\n",
            "poubd\n",
            "yessssss\n",
            "wfwlct\n",
            "waitlist\n",
            "slitterhead\n",
            "lmaoit\n",
            "genderfluid\n",
            "whop\n",
            "csrrying\n",
            "lmaooooo\n",
            "slsjsu\n",
            "parellels\n",
            "casterly\n",
            "fascinado\n",
            "fravor\n",
            "nikkab\n",
            "340gsm\n",
            "chappelles\n",
            "baaki\n",
            "moneyyy\n",
            "bayfuna\n",
            "pie0o\n",
            "etcetctec\n",
            "hatefuck\n",
            "fuckery\n",
            "jeonghan\n",
            "tradies\n",
            "8yrs\n",
            "provoker\n",
            "pndhis\n",
            "stinkpots\n",
            "exoticization\n",
            "age10\n",
            "ambanii\n",
            "fu45\n",
            "x3watch\n",
            "bohot\n",
            "lkhrin\n",
            "mazaak\n",
            "14400\n",
            "remarket\n",
            "phallocentrism\n",
            "replay6\n",
            "s3xy\n",
            "no_tall_men\n",
            "xxr\n",
            "btches\n",
            "movt\n",
            "eavestroughs\n",
            "ouijia\n",
            "bacigalupi\n",
            "chugli\n",
            "simping\n",
            "ladness\n",
            "womensday\n",
            "bellingcat\n",
            "breadtube\n",
            "vaping\n",
            "uvogin\n",
            "incels\n",
            "ranchu\n",
            "tevinter\n",
            "metanarratives\n",
            "transwomen\n",
            "needn\n",
            "librel\n",
            "masla\n",
            "contagiously\n",
            "sexxee\n",
            "weejuns\n",
            "tounge\n",
            "maaaybe\n",
            "vishap\n",
            "cashmeets\n",
            "canva\n",
            "catcall\n",
            "ijāba\n",
            "alice3\n",
            "yzy\n",
            "rwby\n",
            "matriliny\n",
            "peachfuzz\n",
            "blackpilado\n",
            "thinkinglife\n",
            "probs\n",
            "terf\n",
            "voiceit\n",
            "nwxyr\n",
            "lolthe\n",
            "plandemic\n",
            "nagtitake\n",
            "kundrra\n",
            "wh1t3\n",
            "nkiru\n",
            "soakin\n",
            "2022time\n",
            "leafa\n",
            "jokwon\n",
            "324k\n",
            "applebees\n",
            "periodt\n",
            "shivaami\n",
            "lmfaooooo\n",
            "grrls\n",
            "cosettas\n",
            "shanaia\n",
            "voyeuristically\n",
            "tlj\n",
            "yhoo\n",
            "keibo\n",
            "pleople\n",
            "unironically\n",
            "yeezys\n",
            "1time\n",
            "cooky\n",
            "afterpay\n",
            "moolaat\n",
            "estinen\n",
            "zarkashī\n",
            "whitesplaining\n",
            "retweeted\n",
            "appropriative\n",
            "airli\n",
            "duely\n",
            "soys\n",
            "gqp\n",
            "spellground\n",
            "digusting\n",
            "foebut\n",
            "carceral\n",
            "unfahrifhdyfhdh\n",
            "taehyung\n",
            "sholud\n",
            "femdom\n",
            "motherfukers\n",
            "wabr3\n",
            "ironicamente\n",
            "bearmarket\n",
            "omw\n",
            "shouldve\n",
            "itsnotacomplimentmelbourne\n",
            "afaik\n",
            "jikook\n",
            "nonoomf\n",
            "daoibh\n",
            "blowjob\n",
            "idk\n",
            "xx9s\n",
            "kendis\n",
            "gajab\n",
            "duodenary\n",
            "bubs\n",
            "sofoulis\n",
            "lmaooooooo\n",
            "econmy\n",
            "unvaxxed\n",
            "soobin\n",
            "amadour\n",
            "gettinf\n",
            "1size\n",
            "ლ\n",
            "doesnot\n",
            "egotistically\n",
            "unboxing\n",
            "joffy\n",
            "alade\n",
            "enuh\n",
            "altseason\n",
            "chuuyas\n",
            "whiteyes\n",
            "spainisnotademocracy\n",
            "phallogocentrism\n",
            "breebylon\n",
            "sissyslave\n",
            "4get\n",
            "gangbanged\n",
            "arou\n",
            "completly\n",
            "trumpanzee\n",
            "enamor\n",
            "astrodomina\n",
            "thicc\n",
            "liiiife\n",
            "cissexist\n",
            "jyl\n",
            "lollll\n",
            "ohoo\n",
            "goysplaining\n",
            "deltarune\n",
            "underyou\n",
            "incase\n",
            "fe3h\n",
            "ahahhaha\n",
            "leody\n",
            "msut\n",
            "kkab\n",
            "multilocal\n",
            "snowchester\n",
            "patriarcal\n",
            "contrarianism\n",
            "vellem\n",
            "25inch\n",
            "bilocal\n",
            "casiajonna3874\n",
            "xanx\n",
            "mentionnah\n",
            "lolll\n",
            "unrape\n",
            "twitt\n",
            "castrat3d\n",
            "jo1\n",
            "ainihin\n",
            "wyt\n",
            "wanderin\n",
            "soulsmixed\n",
            "femimism\n",
            "yess\n",
            "some1\n",
            "whitesupremacistcisnormativepatriarchy\n",
            "socials10\n",
            "uburu\n",
            "lastwithout\n",
            "gezz\n",
            "deffend\n",
            "alaih\n",
            "selfie\n",
            "ddd06jan\n",
            "fariah\n",
            "downvoted\n",
            "clossus\n",
            "perfs\n",
            "ndumba\n",
            "hermain\n",
            "yoongi\n",
            "pornhub\n",
            "needso\n",
            "fnaf\n",
            "kalfar\n",
            "0r\n",
            "colourways\n",
            "taehyungjimin\n",
            "40x40cms\n",
            "namjoons\n",
            "idgaf\n",
            "alse\n",
            "njäää\n",
            "timehop\n",
            "tf2\n",
            "urbex\n",
            "titsit\n",
            "ruind\n",
            "pple\n",
            "envelope4\n",
            "heckin\n",
            "covid\n",
            "indue\n",
            "aislated\n",
            "drachen\n",
            "lmao\n",
            "shezad\n",
            "cryptocurrency\n",
            "shitstorm\n",
            "squicky\n",
            "beclouded\n",
            "gamergate\n",
            "retweets\n",
            "femboy\n",
            "shld\n",
            "toattack\n",
            "lmfaooo\n",
            "pervs\n",
            "ohmylord\n",
            "spitballing\n",
            "fuckingg\n",
            "uwu\n",
            "surtout\n",
            "113rd\n",
            "whoresrape\n",
            "pardaah\n",
            "bhole\n",
            "30pm\n",
            "emoji\n",
            "switchsportsnotallmenwow\n",
            "ibang\n",
            "tiddies\n",
            "koulchi\n",
            "vaccinated100\n",
            "55k\n",
            "ocding\n",
            "herei\n",
            "whitetrash\n",
            "matriarcal\n",
            "pavard\n",
            "traumatise\n",
            "nsfw\n",
            "legutko\n",
            "dhagne\n",
            "fvck\n",
            "bcuz\n",
            "blocklist\n",
            "ww3\n",
            "ryalls\n",
            "feminazi\n",
            "subserve\n",
            "3dimension\n",
            "42yrs\n",
            "kjvsource\n",
            "piux\n",
            "mfw\n",
            "basmah\n",
            "brexit\n",
            "suça\n",
            "hotwife\n",
            "slutbag\n",
            "conscientização\n",
            "1yrs\n",
            "namjoon\n",
            "bemuse\n",
            "passan\n",
            "becus\n",
            "lmaoooooooo\n",
            "forcely\n",
            "1111super\n",
            "glutenfree\n",
            "bitchesi\n",
            "jeongguk\n",
            "pumuna\n",
            "finna\n",
            "voice10\n",
            "johnleemushr\n",
            "culty\n",
            "septenary\n",
            "kehlani\n",
            "balise\n",
            "44k\n",
            "thise\n",
            "christmas2\n",
            "ujiko\n",
            "cysm\n",
            "hoteps\n",
            "gogos\n",
            "suuuuck\n",
            "shouldnt\n",
            "doctress\n",
            "kikyuuse\n",
            "autogynephiliait\n",
            "mikaelson\n",
            "barelkowski\n",
            "jungkook\n",
            "awashilo\n",
            "manipulatrix\n",
            "twaw\n",
            "femcels\n",
            "omize\n",
            "taec\n",
            "reify\n",
            "yorknew\n",
            "all4guys\n",
            "favs\n",
            "seatcomment\n",
            "patrilocal\n",
            "glanzer\n",
            "catastrophizing\n",
            "nollaigh\n",
            "27any\n",
            "avax\n",
            "mkbl\n",
            "overpeeforming\n",
            "katulad\n",
            "contradictionit\n",
            "cumhole\n",
            "taehyun\n",
            "clit\n",
            "p55x8wjgxk\n",
            "avunlocal\n",
            "ohhhhh\n",
            "angries\n",
            "bakey\n",
            "essentialismdrone\n",
            "guyz\n",
            "brazy\n",
            "montain\n",
            "kinlay\n",
            "2014i\n",
            "nalang\n",
            "hazviite\n",
            "copypaste\n",
            "nuancebecause\n",
            "cnz\n",
            "lolz\n",
            "tejasswi\n",
            "ᅠ\n",
            "womening\n",
            "twirra\n",
            "asapits\n",
            "nervetrainers\n",
            "activismby\n",
            "gojiro7story\n",
            "innesota\n",
            "unting\n",
            "go2\n",
            "undertale\n",
            "pasiko\n",
            "hilig\n",
            "uwinguwi\n",
            "sawayama\n",
            "whyyyyy\n",
            "spitroast\n",
            "werent\n",
            "perjorative\n",
            "debutation\n",
            "7years\n",
            "halward\n",
            "ayleen\n",
            "cumdump\n",
            "akdhajhdajjahshh\n",
            "tigreans\n",
            "newb\n",
            "amogus\n",
            "cintebt\n",
            "impend\n",
            "fastforward\n",
            "peekers\n",
            "kuppalli\n",
            "anddodge\n",
            "brnovich\n",
            "alterlife\n",
            "pplleeaassee\n",
            "deffo\n",
            "russiagate\n",
            "mmyyyy\n",
            "moreid\n",
            "ammm\n",
            "impassion\n",
            "doucheraj\n",
            "cisd5oic\n",
            "rahmatullahi\n",
            "vatansever\n",
            "hoomans\n",
            "tbh\n",
            "hahaha\n",
            "kinju\n",
            "blither\n",
            "incel\n",
            "lmaoo\n",
            "jcj\n",
            "allegationsjudge\n",
            "dogboys\n",
            "ambie\n",
            "doll5\n",
            "titanfall\n",
            "manspreading\n",
            "gtfo\n",
            "slenders\n",
            "crubbs\n",
            "onlyfans\n",
            "slimzosrecordings\n",
            "dooooo\n",
            "urgh\n",
            "ppvs\n",
            "kep1er\n",
            "mainex\n",
            "sequela\n",
            "acephalous\n",
            "fucky\n",
            "jisse\n",
            "stockholme\n",
            "autist\n",
            "hahahaha\n",
            "amasịrị\n",
            "jp2\n",
            "upstander\n",
            "b24\n",
            "indention\n",
            "nutjob\n",
            "longevists\n",
            "cyberwhistle\n",
            "cr7\n",
            "ghisipiti\n",
            "bidmc\n",
            "sanctomony\n",
            "dunsparce\n",
            "rekieta\n",
            "5pcs\n",
            "sjfkkffkgk\n",
            "lazada\n",
            "vawg\n",
            "suburan\n",
            "mgtow\n",
            "jaskier\n",
            "3danimation\n",
            "overshare\n",
            "chocker\n",
            "UNK\n",
            "maskless\n",
            "maaaster\n",
            "denary\n",
            "roblox\n",
            "beomgyu\n",
            "hodlers\n",
            "sexualize\n",
            "woman4\n",
            "muzzie\n",
            "wft\n",
            "unfollowing\n",
            "coutesy\n",
            "kungcono\n",
            "intersexuality\n",
            "andhkaar\n",
            "redpill\n",
            "shithole\n",
            "rlly\n",
            "tianlang\n",
            "endsars\n",
            "adorama\n",
            "ambitionz\n",
            "bestie\n",
            "halfwit\n",
            "pajeet\n",
            "androcentrism\n",
            "overcode\n",
            "premure\n",
            "convo\n",
            "yeaaaaa\n",
            "professionaly\n",
            "neolocal\n",
            "kep1e\n",
            "partynextdoor\n",
            "hyperfocused\n",
            "sfprice\n",
            "femboys\n",
            "innuendosthe\n",
            "yr7\n",
            "alotta\n",
            "tekashi\n",
            "pickme\n",
            "submissiv\n",
            "igboeze\n",
            "shitstain\n",
            "youngkin\n",
            "s01\n",
            "worldbuilding\n",
            "giratina\n",
            "dance11\n",
            "moranbong\n",
            "handemiyy\n",
            "lionza\n",
            "ridah\n",
            "arklight\n",
            "squirter\n",
            "greatful\n",
            "akaabireen\n",
            "i2m\n",
            "marginalian\n",
            "flir1\n",
            "stronk\n",
            "overexaggerate\n",
            "택모닝\n",
            "foilage\n",
            "nevee\n",
            "cheerlead\n",
            "steroided\n",
            "maseehullah\n",
            "diztortion\n",
            "huskiness\n",
            "sexuality10\n",
            "nvm\n",
            "thems\n",
            "hayi\n",
            "ikr\n",
            "rights4\n",
            "mayb\n",
            "web3\n",
            "ofcourse\n",
            "softspun\n",
            "tfumy\n",
            "unfollowed\n",
            "mansplaining\n",
            "souldn\n",
            "wenu\n",
            "4of\n",
            "lunaria\n",
            "radabots\n",
            "femoid\n",
            "cecilt\n",
            "entertainmmt\n",
            "tjs0902\n",
            "makhulu\n",
            "unfollow\n",
            "gobbily\n",
            "roga4them\n",
            "powerwash\n",
            "straigh\n",
            "r1000\n",
            "swic\n",
            "chromatica\n",
            "establiish\n",
            "binarily\n",
            "shrinate\n",
            "sirmed\n",
            "allyship\n",
            "workshoppers\n",
            "spsp\n",
            "ao3\n",
            "in2022\n",
            "evangelilical\n",
            "rrot\n",
            "strocking\n",
            "4pics\n",
            "jungwoo\n",
            "nonbinary\n",
            "mras\n",
            "onea\n",
            "wolflord\n",
            "p5qw1g\n",
            "talaks\n",
            "metanarrative\n",
            "shmeat\n",
            "demiboys\n",
            "feminazis\n",
            "casteist\n",
            "kn95\n",
            "nigcels\n",
            "interacted10\n",
            "nahh\n",
            "alreadyeither\n",
            "kylo\n",
            "dipshit\n",
            "commodifying\n",
            "groupchat\n",
            "reffing\n",
            "wagufi\n",
            "assanger\n",
            "tjprotect\n",
            "bahut\n",
            "mehwish\n",
            "dfkm\n",
            "malchaamah\n",
            "goiz\n",
            "losingyall\n",
            "buttling\n",
            "iterability\n",
            "fuckit\n",
            "antivax\n",
            "sandles\n",
            "unaliving\n",
            "dudebros\n",
            "mfing\n",
            "whoohahaha\n",
            "lunabrooooo\n",
            "problematised\n",
            "goxuan\n",
            "papism\n",
            "ín\n",
            "alresdy\n",
            "masqurading\n",
            "releasecommission\n",
            "terfs\n",
            "intwrvewers\n",
            "dumbass\n",
            "taegi\n",
            "a108m31\n",
            "religi0n\n",
            "reconnaissante\n",
            "dmed10\n",
            "availiaple\n",
            "arely\n",
            "olma\n",
            "brandsma\n",
            "b9aw\n",
            "prvlgd\n",
            "lmaoooo\n",
            "agender\n",
            "gine\n",
            "doinks\n",
            "sumn\n",
            "expereince\n",
            "transmascs\n",
            "fashy\n",
            "priviledge\n",
            "iwca\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "def add_oov_terms_with_batches(embedding_model: KeyedVectors, oov_terms: List[str], vector_size: int = None, batch_size: int = 1000):\n",
        "    vector_size = vector_size or embedding_model.vector_size\n",
        "\n",
        "    # Create a new KeyedVectors object\n",
        "    new_kv = KeyedVectors(vector_size)\n",
        "\n",
        "    # Prepare data for batch addition\n",
        "    words = list(embedding_model.key_to_index.keys()) + oov_terms\n",
        "    vectors = [embedding_model[word] for word in embedding_model.key_to_index] + [np.random.uniform(-0.1, 0.1, vector_size) for _ in oov_terms]\n",
        "\n",
        "    # Add vectors in batches\n",
        "    for i in range(0, len(words), batch_size):\n",
        "        batch_words = words[i:i + batch_size]\n",
        "        batch_vectors = vectors[i:i + batch_size]\n",
        "        new_kv.add_vectors(batch_words, batch_vectors)\n",
        "\n",
        "    return new_kv\n",
        "\n",
        "# Example usage:\n",
        "vector_size = embedding_model.vector_size\n",
        "extended_model = add_oov_terms_with_batches(embedding_model, oov_terms_train, vector_size, batch_size=1000)"
      ],
      "metadata": {
        "id": "CytMrxGeuI0I"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a KeyedVectors object `embedding_model` and a list `oov_terms`\n",
        "extended_model = add_oov_terms_with_batches(embedding_model, oov_terms_train, batch_size=1000)\n",
        "\n",
        "# Verify the size of the new vocabulary\n",
        "print(\"Extended vocabulary size:\", len(extended_model.key_to_index))\n"
      ],
      "metadata": {
        "id": "4TXoBpUQuNoG",
        "outputId": "b122e53d-af9d-410b-9514-a0d15f889558",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extended vocabulary size: 400880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gensim\n",
        "\n",
        "def assign_static_embeddings(oov_terms, embedding_dim):\n",
        "    oov_to_token = {term: f\"[UNK]\" for i, term in enumerate(oov_terms)}\n",
        "\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    static_embeddings = {\n",
        "        token: np.random.uniform(-0.1, 0.1, embedding_dim)\n",
        "        for token in oov_to_token.values()\n",
        "    }\n",
        "\n",
        "    return oov_to_token, static_embeddings\n",
        "\n",
        "embedding_dim = embedding_model.vector_size\n",
        "\n",
        "special_token_test, static_embedding_test = assign_static_embeddings(oov_terms_test, embedding_dim)\n",
        "special_token_val, static_embedding_val = assign_static_embeddings(oov_terms_val, embedding_dim)"
      ],
      "metadata": {
        "id": "kkBM4U5xBTZa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JLnuLGHGAUT"
      },
      "source": [
        "# [Task 4 - 1.0 points] Model definition\n",
        "\n",
        "You are now tasked to define your sexism classifier.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQFI9J-JOfXD"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
        "* You are **free** to experiment with hyper-parameters to define the baseline model.\n",
        "\n",
        "* **Model 1**: add an additional LSTM layer to the Baseline model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Urz5miEWLVge"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras.optimizers import AdamW\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "\n",
        "def getBaselineModel(vocab_size, embedding_dimension, embedding_matrix, n_units = 128):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dimension, weights=embedding_matrix, mask_zero=True, name='encoder_embedding_baseline'),\n",
        "        Bidirectional(LSTM(n_units), return_sequences=False),\n",
        "        Dense(1, activation='sigmoid', kernel_regularizer=l2(0.05)),\n",
        "        #TimeDistributed(Dense(units=len(-----), activation='softmax'), name = 'timedistr_dense_layer')),\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=AdamW(), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def getBaselineModel_mod(vocab_size, embedding_dimension, embedding_matrix):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dimension, weights=embedding_matrix, mask_zero=True, name='encoder_embedding_baseline'),\n",
        "        Bidirectional(LSTM(128, return_sequences=False)),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=AdamW(), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def getModel1(vocab_size, embedding_dimension, embedding_matrix, n_units_1 = 128, n_units_2 = 64 ):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dimension, weights=embedding_matrix, mask_zero=True, name='encoder_embedding_model1'),\n",
        "        Bidirectional(LSTM(n_units_1, return_sequences=True)),\n",
        "        Bidirectional(LSTM(n_units_2), return_sequences=False),\n",
        "        Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer=AdamW(), metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jALc_qYGS2E"
      },
      "source": [
        "### Token to embedding mapping\n",
        "\n",
        "You can follow two approaches for encoding tokens in your classifier.\n",
        "\n",
        "### Work directly with embeddings\n",
        "\n",
        "- Compute the embedding of each input token\n",
        "- Feed the mini-batches of shape (batch_size, # tokens, embedding_dim) to your model\n",
        "\n",
        "### Work with Embedding layer\n",
        "\n",
        "- Encode input tokens to token ids\n",
        "- Define a Embedding layer as the first layer of your model\n",
        "- Compute the embedding matrix of all known tokens (i.e., tokens in your vocabulary)\n",
        "- Initialize the Embedding layer with the computed embedding matrix\n",
        "- You are **free** to set the Embedding layer trainable or not"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                           embedding_dimension: int,\n",
        "                           word_to_idx: Dict[str, int],\n",
        "                           vocab_size: int,\n",
        "                           oov_terms: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        if word == 'UNK':\n",
        "          embedding_matrix[idx] = np.zeros(embedding_dimension)\n",
        "        else:\n",
        "          try:\n",
        "              embedding_vector = embedding_model[word]\n",
        "          except (KeyError, TypeError):\n",
        "              embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "          embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "UHHdpQJ_IBKO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "embedding_dimension = 50\n",
        "embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx_train, len(word_to_idx_train), oov_terms_train)\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
      ],
      "metadata": {
        "id": "yygGuZubH6gx",
        "outputId": "9ee608aa-69a1-4290-b4ae-11bac161ecfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9388/9388 [00:00<00:00, 135257.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding matrix shape: (9388, 50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2159/2159 [00:00<00:00, 233070.87it/s]\n",
            "100%|██████████| 1538/1538 [00:00<00:00, 228259.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Embedding matrix (test set) shape: (2159, 50)\n",
            "Embedding matrix (validation set) shape: (1538, 50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Z44PckrNGfTv"
      },
      "outputs": [],
      "source": [
        "embedding = tf.keras.layers.Embedding(input_dim=len(word_to_idx_train),\n",
        "                                      output_dim=50,                    #embedding dimension\n",
        "                                      weights=[embedding_matrix],\n",
        "                                      mask_zero=True,                   # automatically masks padding tokens\n",
        "                                      name='encoder_embedding')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def convert_tokens_to_indices(tokenized_sentences, word_to_idx, unk_token='UNK'):\n",
        "    unk_index = word_to_idx.get(unk_token, 0)  # Default to 0 if UNK is not in dictionary\n",
        "    sequences = [\n",
        "        [word_to_idx.get(token, unk_index) for token in sentence]\n",
        "        for sentence in tokenized_sentences\n",
        "    ]\n",
        "    return sequences\n",
        "\n",
        "# 1. Convert Tokenized Sentences to Indices\n",
        "tokenized_sentences = df_train['cleaned_tweet'].tolist()  # Replace with your dataframe column\n",
        "sequences = convert_tokens_to_indices(tokenized_sentences, word_to_idx_train)\n",
        "\n",
        "# 2. Pad Sequences\n",
        "max_sequence_length = 50  # Adjust based on your dataset or experiment with different lengths\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Now padded_sequences is ready to be used as input for training\n",
        "print(\"Shape of padded sequences:\", np.shape(padded_sequences))\n",
        "\n",
        "tokenized_sentences_val = df_val['cleaned_tweet'].tolist()  # Replace with your dataframe column\n",
        "sequences_val = convert_tokens_to_indices(tokenized_sentences_val, word_to_idx_val)\n",
        "\n",
        "# 2. Pad Sequences\n",
        "max_sequence_length = 50  # Adjust based on your dataset or experiment with different lengths\n",
        "padded_sequences_val = pad_sequences(sequences_val, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Now padded_sequences is ready to be used as input for training\n",
        "print(\"Shape of padded sequences:\", np.shape(padded_sequences_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5RPgW9V0p58",
        "outputId": "08ec5423-6953-4cb6-f0c9-21e71f8499c5"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of padded sequences: (2870, 50)\n",
            "Shape of padded sequences: (158, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEQTPu6eGgGv"
      },
      "source": [
        "### Padding\n",
        "\n",
        "Pay attention to padding tokens!\n",
        "\n",
        "Your model **should not** be penalized on those tokens.\n",
        "\n",
        "#### How to?\n",
        "\n",
        "There are two main ways.\n",
        "\n",
        "However, their implementation depends on the neural library you are using.\n",
        "\n",
        "- Embedding layer\n",
        "- Custom loss to compute average cross-entropy on non-padding tokens only\n",
        "\n",
        "**Note**: This is a **recommendation**, but we **do not penalize** for missing workarounds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFjBgdiRG3wD"
      },
      "source": [
        "# [Task 5 - 1.0 points] Training and Evaluation\n",
        "\n",
        "You are now tasked to train and evaluate the Baseline and Model 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWPK4umGOjtT"
      },
      "source": [
        "\n",
        "### Instructions\n",
        "\n",
        "* Train **all** models on the train set.\n",
        "* Evaluate **all** models on the validation set.\n",
        "* Compute metrics on the validation set.\n",
        "* Pick **at least** three seeds for robust estimation.\n",
        "* Pick the **best** performing model according to the observed validation set performance.\n",
        "* Evaluate your models using macro F1-score."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_train, X_val, y_train, y_val are already prepared\n",
        "X_train = padded_sequences # Sequences of token indices for training\n",
        "y_train = df_train['hard_label_task1']  # Binary labels for training\n",
        "X_val = padded_sequences_val  # Sequences of token indices for validation\n",
        "y_val = df_val['hard_label_task1']    # Binary labels for validation\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# Get BaselineModel\n",
        "vocab_size = len(word_to_idx_train)\n",
        "baseline_model = getBaselineModel_mod(vocab_size, embedding_dimension, [embedding_matrix])\n",
        "\n",
        "# Train BaselineModel\n",
        "history_baseline = baseline_model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "# Get Model1\n",
        "model1 = getModel1(vocab_size, embedding_dimension, [embedding_matrix])\n",
        "\n",
        "# Train Model1\n",
        "history_model1 = model1.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "id": "E7mdsM5XXgHD",
        "outputId": "65b79558-c6d4-46f4-857e-cc23e871180e"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.5697 - loss: 0.6890 - val_accuracy: 0.5696 - val_loss: 0.6977\n",
            "Epoch 2/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6014 - loss: 0.6723 - val_accuracy: 0.5696 - val_loss: 0.7114\n",
            "Epoch 3/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5858 - loss: 0.6698 - val_accuracy: 0.5696 - val_loss: 0.7245\n",
            "Epoch 4/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6052 - loss: 0.6648 - val_accuracy: 0.5696 - val_loss: 0.7571\n",
            "Epoch 5/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6107 - loss: 0.6575 - val_accuracy: 0.5696 - val_loss: 0.7396\n",
            "Epoch 6/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5989 - loss: 0.6638 - val_accuracy: 0.5696 - val_loss: 0.7802\n",
            "Epoch 7/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.6320 - loss: 0.6430 - val_accuracy: 0.5696 - val_loss: 0.8353\n",
            "Epoch 8/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.6280 - loss: 0.6443 - val_accuracy: 0.5696 - val_loss: 0.7384\n",
            "Epoch 9/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6379 - loss: 0.6438 - val_accuracy: 0.5696 - val_loss: 0.7698\n",
            "Epoch 10/10\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6246 - loss: 0.6443 - val_accuracy: 0.5696 - val_loss: 0.7893\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unrecognized keyword arguments passed to Bidirectional: {'return_sequences': False}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-ffc19228f594>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Get Model1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetModel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Train Model1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-320db9ee5357>\u001b[0m in \u001b[0;36mgetModel1\u001b[0;34m(vocab_size, embedding_dimension, embedding_matrix, n_units_1, n_units_2)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_zero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'encoder_embedding_model1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_units_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_units_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     ])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/bidirectional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layer, merge_mode, weights, backward_layer, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;34m'{\"sum\", \"mul\", \"ave\", \"concat\", None}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             )\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Recreate the forward layer from the original layer config, so that it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_shape_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape_arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    286\u001b[0m                 \u001b[0;34m\"Unrecognized keyword arguments \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;34mf\"passed to {self.__class__.__name__}: {kwargs}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Bidirectional: {'return_sequences': False}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSy9sPwYHUoD"
      },
      "source": [
        "# [Task 6 - 1.0 points] Transformers\n",
        "\n",
        "In this section, you will use a transformer model specifically trained for hate speech detection, namely [Twitter-roBERTa-base for Hate Speech Detection](https://huggingface.co/cardiffnlp/twitter-roberta-base-hate).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "KIs_Fg5oLVgf"
      },
      "source": [
        "### Relevant Material\n",
        "- Tutorial 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "qE0wYj0wLVgf"
      },
      "source": [
        "### Instructions\n",
        "1. **Load the Tokenizer and Model**\n",
        "\n",
        "2. **Preprocess the Dataset**:\n",
        "   You will need to preprocess your dataset to prepare it for input into the model. Tokenize your text data using the appropriate tokenizer and ensure it is formatted correctly.\n",
        "\n",
        "   **Note**: You have to use the plain text of the dataset and not the version that you tokenized before, as you need to tokenize the cleaned text obtained after the initial cleaning process.\n",
        "\n",
        "3. **Train the Model**:\n",
        "   Use the `Trainer` to train the model on your training data.\n",
        "\n",
        "4. **Evaluate the Model on the Test Set** using F1-macro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gtiG2mAL3HM"
      },
      "source": [
        "# [Task 7 - 0.5 points] Error Analysis\n",
        "\n",
        "### Instructions\n",
        "\n",
        "After evaluating the model, perform a brief error analysis:\n",
        "\n",
        " - Review the results and identify common errors.\n",
        "\n",
        " - Summarize your findings regarding the errors and their impact on performance (e.g. but not limited to Out-of-Vocabulary (OOV) words, data imbalance, and performance differences between the custom model and the transformer...)\n",
        " - Suggest possible solutions to address the identified errors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P42XYjb6K3k5"
      },
      "source": [
        "# [Task 8 - 0.5 points] Report\n",
        "\n",
        "Wrap up your experiment in a short report (up to 2 pages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9oXSaW1K5S7"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Use the NLP course report template.\n",
        "* Summarize each task in the report following the provided template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHw2L6PlLDyE"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "The report is not a copy-paste of graphs, tables, and command outputs.\n",
        "\n",
        "* Summarize classification performance in Table format.\n",
        "* **Do not** report command outputs or screenshots.\n",
        "* Report learning curves in Figure format.\n",
        "* The error analysis section should summarize your findings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMUqh1utLflM"
      },
      "source": [
        "# Submission\n",
        "\n",
        "* **Submit** your report in PDF format.\n",
        "* **Submit** your python notebook.\n",
        "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
        "* You can upload **model weights** in a cloud repository and report the link in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypagJed7LheY"
      },
      "source": [
        "# FAQ\n",
        "\n",
        "Please check this frequently asked questions before contacting us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgtFwKXMLjww"
      },
      "source": [
        "### Execution Order\n",
        "\n",
        "You are **free** to address tasks in any order (if multiple orderings are available)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BjMk5e_M4n7"
      },
      "source": [
        "### Trainable Embeddings\n",
        "\n",
        "You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8TVgpYlM6s5"
      },
      "source": [
        "### Model architecture\n",
        "\n",
        "You **should not** change the architecture of a model (i.e., its layers).\n",
        "However, you are **free** to play with their hyper-parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia6IapI1M_A7"
      },
      "source": [
        "### Neural Libraries\n",
        "\n",
        "You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWDaW8DyNBu5"
      },
      "source": [
        "### Keras TimeDistributed Dense layer\n",
        "\n",
        "If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1WcrpemNEQm"
      },
      "source": [
        "### Robust Evaluation\n",
        "\n",
        "Each model is trained with at least 3 random seeds.\n",
        "\n",
        "Task 4 requires you to compute the average performance over the 3 seeds and its corresponding standard deviation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mVe5dqzNI_u"
      },
      "source": [
        "### Model Selection for Analysis\n",
        "\n",
        "To carry out the error analysis you are **free** to either\n",
        "\n",
        "* Pick examples or perform comparisons with an individual seed run model (e.g., Baseline seed 1337)\n",
        "* Perform ensembling via, for instance, majority voting to obtain a single model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8a4pDKSNKzI"
      },
      "source": [
        "### Error Analysis\n",
        "\n",
        "Some topics for discussion include:\n",
        "   * Precision/Recall curves.\n",
        "   * Confusion matrices.\n",
        "   * Specific misclassified samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ctL6rGCOLVgk"
      },
      "source": [
        "### Bonus Points\n",
        "Bonus points are arbitrarily assigned based on significant contributions such as:\n",
        "- Outstanding error analysis\n",
        "- Masterclass code organization\n",
        "- Suitable extensions\n",
        "Note that bonus points are only assigned if all task points are attributed (i.e., 6/6).\n",
        "\n",
        "**Possible Extensions/Explorations for Bonus Points:**\n",
        "- **Try other preprocessing strategies**: e.g., but not limited to, explore techniques tailored specifically for tweets or  methods that are common in social media text.\n",
        "- **Experiment with other custom architectures or models from HuggingFace**\n",
        "- **Explore Spanish tweets**: e.g., but not limited to, leverage multilingual models to process Spanish tweets and assess their performance compared to monolingual models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xmMKE7vLu-y"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# The End"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}